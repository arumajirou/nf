{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9dcc2e",
   "metadata": {},
   "source": [
    "\n",
    "# NeuralForecast 分析テーブル・ビューワ（JupyterLab用）\n",
    "\n",
    "このノートブックは、`nf_auto_runs/analysis/` に出力された **統合Excel**（`model_analysis_*.xlsx`）や、\n",
    "**個別CSV**（`model_profile_*.csv` など）を自動検出して読み込み、JupyterLab 上で閲覧・検索・簡易集計できるようにします。\n",
    "\n",
    "- まずは **パス設定** セルの `SEARCH_DIRS` に、実際の分析ファイル（Excel/CSV）があるフォルダを指定してください（複数指定可）。\n",
    "- 統合Excelがあれば優先して読み込み、足りない表はCSVから補完します。\n",
    "- `ipywidgets` がインストール済みなら、ドロップダウンで表を切り替えて検索・抽出できます。\n",
    "- `ipywidgets` が無い場合は、フォールバックとして静的プレビューを表示します。\n",
    "\n",
    "> 想定ファイル名の例：  \n",
    "> `model_analysis_20251111_161847.xlsx`  \n",
    "> `model_profile_20251111_161847.csv`, `dataset_profile_*.csv`, `training_state_*.csv`,  \n",
    "> `weight_statistics_*.csv`, `model_complexity_*.csv`, `model_diagnosis_*.csv`,  \n",
    "> `parameter_sensitivity_*.csv`, `optimization_suggestions_*.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a33449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# === パス設定（必要に応じて編集）==============================================\n",
    "# 分析ファイル（Excel/CSV）が存在する候補ディレクトリをリストに入れてください。\n",
    "SEARCH_DIRS = [\n",
    "    Path.cwd() / \"nf_auto_runs\" / \"analysis\",\n",
    "    Path.cwd(),                       # 現在の作業ディレクトリ直下\n",
    "    Path.home() / \"Downloads\",        # ダウンロードフォルダ（必要なら）\n",
    "]\n",
    "\n",
    "# 追加で明示的に指定したいパスがあれば↓を編集\n",
    "# SEARCH_DIRS.append(Path(r\"C:\\Users\\YOUR_NAME\\path\\to\\nf_auto_runs\\analysis\"))\n",
    "# ============================================================================\n",
    "\n",
    "# 検索対象のファイル名パターン\n",
    "EXCEL_PATTERN = \"model_analysis_*.xlsx\"\n",
    "CSV_PATTERNS = {\n",
    "    \"model_profile\": \"*model_profile_*.csv\",\n",
    "    \"dataset_profile\": \"*dataset_profile_*.csv\",\n",
    "    \"training_state\": \"*training_state_*.csv\",\n",
    "    \"weight_statistics\": \"*weight_statistics_*.csv\",\n",
    "    \"model_complexity\": \"*model_complexity_*.csv\",\n",
    "    \"parameter_sensitivity\": \"*parameter_sensitivity_*.csv\",\n",
    "    \"model_diagnosis\": \"*model_diagnosis_*.csv\",\n",
    "    \"optimization_suggestions\": \"*optimization_suggestions_*.csv\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af90116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "\n",
    "def glob_all(pattern: str, roots: List[Path]) -> List[Path]:\n",
    "    hits: List[Path] = []\n",
    "    for r in roots:\n",
    "        if r.exists():\n",
    "            hits.extend(r.rglob(pattern))\n",
    "    # 重複除去（解決後の絶対パスで比較）\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for p in hits:\n",
    "        try:\n",
    "            key = p.resolve()\n",
    "        except Exception:\n",
    "            key = p\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append(p)\n",
    "    return uniq\n",
    "\n",
    "def find_latest(paths: List[Path]) -> Optional[Path]:\n",
    "    paths = [p for p in paths if p.exists()]\n",
    "    if not paths:\n",
    "        return None\n",
    "    return sorted(paths, key=lambda p: p.stat().st_mtime, reverse=True)[0]\n",
    "\n",
    "def try_read_csv(path: Path) -> pd.DataFrame:\n",
    "    # 文字コードを総当たり（日本語CSV対策）\n",
    "    encodings = [\"utf-8\", \"utf-8-sig\", \"cp932\", \"latin-1\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "def load_from_excel(xlsx_path: Path) -> Dict[str, pd.DataFrame]:\n",
    "    dfs: Dict[str, pd.DataFrame] = {}\n",
    "    try:\n",
    "        xls = pd.ExcelFile(xlsx_path)  # openpyxlが必要\n",
    "    except Exception as e:\n",
    "        print(f\"[info] Excel読み込みに失敗: {e}\\n-> 'pip install openpyxl' を試してください。\")\n",
    "        return dfs\n",
    "    for sheet in xls.sheet_names:\n",
    "        try:\n",
    "            df = pd.read_excel(xls, sheet_name=sheet)\n",
    "            key = sheet.strip().lower()\n",
    "            key = re.sub(r\"\\s+\", \"_\", key)\n",
    "            dfs[key] = df\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] シート '{sheet}' 読み込みエラー: {e}\")\n",
    "    return dfs\n",
    "\n",
    "def load_from_csvs(roots: List[Path]) -> Dict[str, pd.DataFrame]:\n",
    "    dfs: Dict[str, pd.DataFrame] = {}\n",
    "    for key, pattern in CSV_PATTERNS.items():\n",
    "        cands = glob_all(pattern, roots)\n",
    "        if not cands:\n",
    "            continue\n",
    "        latest = find_latest(cands)\n",
    "        if latest is None:\n",
    "            continue\n",
    "        try:\n",
    "            df = try_read_csv(latest)\n",
    "            dfs[key] = df\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] {key} CSV読込失敗 ({latest}): {e}\")\n",
    "    return dfs\n",
    "\n",
    "def memory_usage_mb(df: pd.DataFrame) -> float:\n",
    "    return float(df.memory_usage(index=True, deep=True).sum() / (1024 ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb16b15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] 統合Excelを検出: c:\\Users\\hashimoto.ryohei\\Downloads\\zip\\sonnet\\nf_auto_runs\\analysis\\model_analysis_20251111_161847.xlsx\n",
      "=== 読み込んだテーブル一覧 ===\n",
      "- model_profile: 1 行 × 9 列, 約 0.00 MB\n",
      "- dataset_profile: 1 行 × 11 列, 約 0.00 MB\n",
      "- training_state: 1 行 × 7 列, 約 0.00 MB\n",
      "- weight_statistics: 26 行 × 13 列, 約 0.01 MB\n",
      "- model_complexity: 1 行 × 6 列, 約 0.00 MB\n",
      "- model_diagnosis: 1 行 × 5 列, 約 0.00 MB\n",
      "- optimization_suggestions: 1 行 × 7 列, 約 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 統合Excel → CSVの順で読み込み（Excelがあれば優先）\n",
    "excel_files = glob_all(EXCEL_PATTERN, SEARCH_DIRS)\n",
    "latest_excel = find_latest(excel_files)\n",
    "\n",
    "tables: Dict[str, pd.DataFrame] = {}\n",
    "if latest_excel:\n",
    "    print(f\"[info] 統合Excelを検出: {latest_excel}\")\n",
    "    tables = load_from_excel(latest_excel)\n",
    "\n",
    "# CSVで補完（あるいはExcelが無い場合はCSVのみ）\n",
    "csv_tables = load_from_csvs(SEARCH_DIRS)\n",
    "# Excel側を優先しつつ、存在しないキーはCSVで補完\n",
    "for k, v in csv_tables.items():\n",
    "    if k not in tables:\n",
    "        tables[k] = v\n",
    "\n",
    "if not tables:\n",
    "    print(\"\"\"[not found] 分析ファイルが見つかりません。\n",
    "以下のいずれかを配置してください：\n",
    "- 統合Excel: model_analysis_YYYYMMDD_HHMMSS.xlsx\n",
    "- 個別CSV: model_profile_*.csv, dataset_profile_*.csv, training_state_*.csv,\n",
    "            weight_statistics_*.csv, model_complexity_*.csv, model_diagnosis_*.csv,\n",
    "            parameter_sensitivity_*.csv, optimization_suggestions_*.csv\n",
    "    \"\"\".strip())\n",
    "else:\n",
    "    print(\"=== 読み込んだテーブル一覧 ===\")\n",
    "    for key, df in tables.items():\n",
    "        print(f\"- {key}: {len(df)} 行 × {len(df.columns)} 列, 約 {memory_usage_mb(df):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde03e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c93ac37d1d40aaa46b694cd6599980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Table:', layout=Layout(width='300px'), options=('dataset_p…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 対話ビュー（ipywidgets）。未インストールでもこのセルは無害です。\n",
    "try:\n",
    "    import ipywidgets as W\n",
    "    from IPython.display import display, clear_output\n",
    "\n",
    "    if tables:\n",
    "        table_names = sorted(tables.keys())\n",
    "\n",
    "        dd_table = W.Dropdown(options=table_names, description='Table:', layout=W.Layout(width='300px'))\n",
    "        txt_query = W.Text(value='', description='検索:', placeholder='部分一致（列名・値）')\n",
    "        ms_cols   = W.SelectMultiple(options=[], description='列フィルタ:', layout=W.Layout(height='140px', width='300px'))\n",
    "        num_head  = W.IntSlider(value=50, min=5, max=1000, step=5, description='先頭N行:')\n",
    "        btn_show  = W.Button(description='表示', button_style='primary')\n",
    "        out       = W.Output()\n",
    "\n",
    "        def refresh_columns(*args):\n",
    "            df = tables.get(dd_table.value)\n",
    "            if df is not None:\n",
    "                ms_cols.options = list(df.columns)\n",
    "        dd_table.observe(refresh_columns, names='value')\n",
    "        refresh_columns()\n",
    "\n",
    "        def search_mask(df: pd.DataFrame, q: str):\n",
    "            if not q:\n",
    "                return pd.Series([True]*len(df), index=df.index)\n",
    "            q = q.strip().lower()\n",
    "            # 列名一致 or 値の部分一致（文字列化）\n",
    "            col_hit = [c for c in df.columns if q in c.lower()]\n",
    "            mask_val = df.apply(lambda col: col.astype(str).str.lower().str.contains(q, na=False) if col.dtype != 'object' else col.astype(str).str.lower().str.contains(q, na=False))\n",
    "            row_mask = mask_val.any(axis=1)\n",
    "            if col_hit:\n",
    "                return row_mask | pd.Series([True]*len(df), index=df.index)  # 列名ヒットなら全行表示\n",
    "            return row_mask\n",
    "\n",
    "        def on_show_clicked(b):\n",
    "            with out:\n",
    "                clear_output()\n",
    "                df = tables.get(dd_table.value)\n",
    "                if df is None:\n",
    "                    print(\"テーブルが見つかりません。\")\n",
    "                    return\n",
    "\n",
    "                # 列フィルタ\n",
    "                cols = list(ms_cols.value) if ms_cols.value else list(df.columns)\n",
    "                df2 = df.loc[:, cols]\n",
    "\n",
    "                # 検索\n",
    "                q = txt_query.value\n",
    "                mask = search_mask(df2, q)\n",
    "                dfv = df2[mask].head(int(num_head.value))\n",
    "\n",
    "                print(f\"[info] {dd_table.value}: {len(df2)} 行 × {len(df2.columns)} 列 -> フィルタ後 {len(dfv)} 行を表示\")\n",
    "                display(dfv)\n",
    "\n",
    "                # 列ごとの基本統計（数値列のみ）\n",
    "                num_cols = df2.select_dtypes(include=['number']).columns.tolist()\n",
    "                if num_cols:\n",
    "                    print(\"\\n[stats] 数値列の要約（head() の範囲とは独立）\")\n",
    "                    display(df2[num_cols].describe())\n",
    "\n",
    "        btn_show.on_click(on_show_clicked)\n",
    "\n",
    "        ui = W.VBox([\n",
    "            W.HBox([dd_table, num_head]),\n",
    "            W.HBox([txt_query, ms_cols]),\n",
    "            btn_show,\n",
    "            out\n",
    "        ])\n",
    "        display(ui)\n",
    "    else:\n",
    "        print(\"ロード済みテーブルが無いため、対話ビューはスキップします。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[info] ipywidgets が利用できないため、対話ビューをスキップします: {e}\\n\"\n",
    "          f\"-> 'pip install ipywidgets' 実行後、JupyterLab を再読み込みしてください。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e2c0a7",
   "metadata": {},
   "source": [
    "\n",
    "## 非対話プレビュー（必要に応じて実行）\n",
    "\n",
    "以下のユーティリティで、対話ビューなしでもテーブルの概要を確認できます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563c1753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== model_profile ===\n",
      "1 行 × 9 列, 約 0.00 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_dir_hash</th>\n",
       "      <th>model_alias</th>\n",
       "      <th>model_class</th>\n",
       "      <th>h</th>\n",
       "      <th>input_size</th>\n",
       "      <th>freq</th>\n",
       "      <th>total_params</th>\n",
       "      <th>trainable_params</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fde700f070a72ac01e0ecf3df9a2b6946e929f6b92b8ba...</td>\n",
       "      <td>dir(data_long)_parent_dir(N_features)_loto(num...</td>\n",
       "      <td>autotcn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model_dir_hash  \\\n",
       "0  fde700f070a72ac01e0ecf3df9a2b6946e929f6b92b8ba...   \n",
       "\n",
       "                                         model_alias model_class   h  \\\n",
       "0  dir(data_long)_parent_dir(N_features)_loto(num...     autotcn NaN   \n",
       "\n",
       "   input_size  freq  total_params  trainable_params hyperparameters  \n",
       "0         NaN   NaN             0                 0              {}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[info()]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   model_dir_hash    1 non-null      object \n",
      " 1   model_alias       1 non-null      object \n",
      " 2   model_class       1 non-null      object \n",
      " 3   h                 0 non-null      float64\n",
      " 4   input_size        0 non-null      float64\n",
      " 5   freq              0 non-null      float64\n",
      " 6   total_params      1 non-null      int64  \n",
      " 7   trainable_params  1 non-null      int64  \n",
      " 8   hyperparameters   1 non-null      object \n",
      "dtypes: float64(3), int64(2), object(4)\n",
      "memory usage: 204.0+ bytes\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "layer_name\n",
       "hist_encoder.tcn.0.conv.weight            1\n",
       "hist_encoder.tcn.0.conv.bias              1\n",
       "hist_encoder.tcn.0.causalconv.0.weight    1\n",
       "hist_encoder.tcn.0.causalconv.0.bias      1\n",
       "hist_encoder.tcn.1.conv.weight            1\n",
       "hist_encoder.tcn.1.conv.bias              1\n",
       "hist_encoder.tcn.1.causalconv.0.weight    1\n",
       "hist_encoder.tcn.1.causalconv.0.bias      1\n",
       "hist_encoder.tcn.2.conv.weight            1\n",
       "hist_encoder.tcn.2.conv.bias              1\n",
       "hist_encoder.tcn.2.causalconv.0.weight    1\n",
       "hist_encoder.tcn.2.causalconv.0.bias      1\n",
       "hist_encoder.tcn.3.conv.weight            1\n",
       "hist_encoder.tcn.3.conv.bias              1\n",
       "hist_encoder.tcn.3.causalconv.0.weight    1\n",
       "hist_encoder.tcn.3.causalconv.0.bias      1\n",
       "hist_encoder.tcn.4.conv.weight            1\n",
       "hist_encoder.tcn.4.conv.bias              1\n",
       "hist_encoder.tcn.4.causalconv.0.weight    1\n",
       "hist_encoder.tcn.4.causalconv.0.bias      1\n",
       "context_adapter.weight                    1\n",
       "context_adapter.bias                      1\n",
       "mlp_decoder.layers.0.weight               1\n",
       "mlp_decoder.layers.0.bias                 1\n",
       "mlp_decoder.layers.3.weight               1\n",
       "mlp_decoder.layers.3.bias                 1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def show_table(name: str, head_n: int = 20):\n",
    "    df = tables.get(name)\n",
    "    if df is None:\n",
    "        print(f\"'{name}' は読み込まれていません。\")\n",
    "        return\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"{len(df)} 行 × {len(df.columns)} 列, 約 {memory_usage_mb(df):.2f} MB\")\n",
    "    display(df.head(head_n))\n",
    "    print(\"\\n[info()]\")\n",
    "    print(df.info())\n",
    "\n",
    "def show_value_counts(name: str, column: str, top_n: int = 20):\n",
    "    df = tables.get(name)\n",
    "    if df is None:\n",
    "        print(f\"'{name}' は読み込まれていません。\")\n",
    "        return\n",
    "    if column not in df.columns:\n",
    "        print(f\"列 '{column}' は存在しません。候補: {list(df.columns)[:10]} ...\")\n",
    "        return\n",
    "    vc = df[column].astype(str).value_counts().head(top_n)\n",
    "    display(vc)\n",
    "\n",
    "# 例：\n",
    "show_table('model_profile', head_n=10)\n",
    "show_value_counts('weight_statistics', 'layer_name', top_n=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaiseki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
