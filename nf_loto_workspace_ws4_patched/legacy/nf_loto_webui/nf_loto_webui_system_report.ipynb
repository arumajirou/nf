{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NeuralForecast 時系列 MLOps 拡張レポートノートブック\n",
        "\n",
        "このノートブックでは、今回追加した以下のコンポーネントについて、\n",
        "\n",
        "- ディレクトリ構成と役割の解説\n",
        "- 代表的な API の実行・動作確認\n",
        "- メタデータテーブル（DDL）の確認\n",
        "- HTML レポートなどの成果物確認\n",
        "- （オプション）pytest による一括テスト実行\n",
        "\n",
        "を **段階的に** 実行しながら確認できるように構成しています。\n",
        "\n",
        "対象モジュール:\n",
        "\n",
        "- `src/nf_logging/`\n",
        "  - `wandb_logger.py`\n",
        "  - `mlflow_logger.py`\n",
        "- `src/nf_monitoring/`\n",
        "  - `prometheus_metrics.py`\n",
        "  - `resource_monitor.py`\n",
        "- `src/nf_analysis/`\n",
        "  - `drift.py`\n",
        "  - `anomaly.py`\n",
        "  - `model_stats.py`\n",
        "- `src/nf_reports/`\n",
        "  - `html_reporter.py`\n",
        "- `src/nf_metadata/`\n",
        "  - `schema_definitions.py`\n",
        "- `sql/002_extend_metadata_tables.sql`\n",
        "- `tests/` 以下の各テストモジュール\n",
        "\n",
        "このノートブックは **プロジェクトルート直下**（例: `C:\\\\nf\\\\nf_loto_webui`）に置いて実行する想定です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 環境セットアップ（パス設定）\n",
        "\n",
        "まず、`src/` を `sys.path` に追加し、追加したモジュール群をインポートできるようにします。\n",
        "\n",
        "- 通常は、このノートブックをプロジェクトルートに置いた状態で `PROJECT_ROOT = Path.cwd()` のままで動きます。\n",
        "- もし別ディレクトリに置く場合は、`PROJECT_ROOT` を明示的に書き換えてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_ROOT: c:\\nf\\nf_loto_webui\n",
            "SRC_ROOT    : c:\\nf\\nf_loto_webui\\src\n",
            "TESTS_ROOT  : c:\\nf\\nf_loto_webui\\tests\n",
            "SQL_ROOT    : c:\\nf\\nf_loto_webui\\sql\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# ▼必要に応じて書き換え▼\n",
        "# プロジェクトルートをこのノートブックのカレントディレクトリとみなす\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "\n",
        "# 例: 固定で指定したい場合（Windows）\n",
        "# PROJECT_ROOT = Path(r\"C:\\nf\\nf_loto_webui\")\n",
        "\n",
        "SRC_ROOT = PROJECT_ROOT / \"src\"\n",
        "TESTS_ROOT = PROJECT_ROOT / \"tests\"\n",
        "SQL_ROOT = PROJECT_ROOT / \"sql\"\n",
        "\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"SRC_ROOT    :\", SRC_ROOT)\n",
        "print(\"TESTS_ROOT  :\", TESTS_ROOT)\n",
        "print(\"SQL_ROOT    :\", SQL_ROOT)\n",
        "\n",
        "if str(SRC_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_ROOT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 追加ディレクトリ・ファイル構成の確認\n",
        "\n",
        "まずは、今回追加した主なディレクトリ・ファイルが存在するかを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\nf\\nf_loto_webui\\src\\nf_logging\\wandb_logger.py -> OK\n",
            "c:\\nf\\nf_loto_webui\\src\\nf_logging\\mlflow_logger.py -> OK\n",
            "c:\\nf\\nf_loto_webui\\src\\nf_monitoring\\prometheus_metrics.py -> OK\n",
            "c:\\nf\\nf_loto_webui\\src\\nf_monitoring\\resource_monitor.py -> OK\n",
            "c:\\nf\\nf_loto_webui\\src\\nf_analysis\\drift.py -> OK\n",
            "c:\\nf\\nf_loto_webui\\src\\nf_analysis\\anomaly.py -> OK\n",
            "c:\\nf\\nf_loto_webui\\src\\nf_analysis\\model_stats.py -> OK\n",
            "c:\\nf\\nf_loto_webui\\src\\nf_reports\\html_reporter.py -> OK\n",
            "c:\\nf\\nf_loto_webui\\src\\nf_metadata\\schema_definitions.py -> OK\n",
            "c:\\nf\\nf_loto_webui\\sql\\002_extend_metadata_tables.sql -> OK\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "expected_paths = [\n",
        "    SRC_ROOT / \"nf_logging\" / \"wandb_logger.py\",\n",
        "    SRC_ROOT / \"nf_logging\" / \"mlflow_logger.py\",\n",
        "    SRC_ROOT / \"nf_monitoring\" / \"prometheus_metrics.py\",\n",
        "    SRC_ROOT / \"nf_monitoring\" / \"resource_monitor.py\",\n",
        "    SRC_ROOT / \"nf_analysis\" / \"drift.py\",\n",
        "    SRC_ROOT / \"nf_analysis\" / \"anomaly.py\",\n",
        "    SRC_ROOT / \"nf_analysis\" / \"model_stats.py\",\n",
        "    SRC_ROOT / \"nf_reports\" / \"html_reporter.py\",\n",
        "    SRC_ROOT / \"nf_metadata\" / \"schema_definitions.py\",\n",
        "    SQL_ROOT / \"002_extend_metadata_tables.sql\",\n",
        "]\n",
        "\n",
        "for path in expected_paths:\n",
        "    print(f\"{path} -> {'OK' if path.exists() else 'MISSING'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ロギングレイヤの動作確認\n",
        "\n",
        "ここでは、`nf_logging` の W&B ロガー / MLflow ロガーを簡単に動かしてみます。\n",
        "\n",
        "ポイント:\n",
        "\n",
        "- 依存ライブラリ（`wandb`, `mlflow`）が **未インストールでも安全に no-op** になる設計です。\n",
        "- 環境変数で `NF_WANDB_ENABLED`, `NF_MLFLOW_ENABLED` を `1` にすると実際に連携を有効化できます。\n",
        "- このノートブックでは安全のため **デフォルトで無効** の動作を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== W&B ロガー（無効モード） ===\n",
            "ctx.enabled: False\n",
            "W&B dummy run finished.\n",
            "\n",
            "=== MLflow ロガー（無効モード） ===\n",
            "MLflow run object: None\n",
            "MLflow dummy context exited.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from nf_logging import wandb_logger, mlflow_logger\n",
        "\n",
        "# 念のため無効化しておく\n",
        "os.environ[\"NF_WANDB_ENABLED\"] = \"0\"\n",
        "os.environ[\"NF_MLFLOW_ENABLED\"] = \"0\"\n",
        "\n",
        "print(\"=== W&B ロガー（無効モード） ===\")\n",
        "ctx = wandb_logger.start_wandb_run(\n",
        "    enabled=None,\n",
        "    project=\"demo-project\",\n",
        "    run_name=\"demo-run\",\n",
        "    config={\"demo\": True},\n",
        ")\n",
        "print(\"ctx.enabled:\", ctx.enabled)\n",
        "ctx.log_metrics({\"demo_loss\": 0.123}, step=1)\n",
        "ctx.set_summary({\"status\": \"ok\"})\n",
        "ctx.finish()\n",
        "print(\"W&B dummy run finished.\\n\")\n",
        "\n",
        "print(\"=== MLflow ロガー（無効モード） ===\")\n",
        "with mlflow_logger.mlflow_run_context(\n",
        "    enabled=None,\n",
        "    run_name=\"demo-mlflow-run\",\n",
        "    experiment_name=\"demo-experiment\",\n",
        "    tags={\"purpose\": \"demo\"},\n",
        "    params={\"a\": 1},\n",
        ") as run:\n",
        "    print(\"MLflow run object:\", run)\n",
        "print(\"MLflow dummy context exited.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "上記のセルでは、`ctx.enabled` が `False` になっていれば、依存ライブラリに関わらず安全に no-op で動作していることが確認できます。\n",
        "\n",
        "実際に W&B / MLflow にログを送りたい場合は、それぞれ\n",
        "\n",
        "```bash\n",
        "set NF_WANDB_ENABLED=1\n",
        "set NF_MLFLOW_ENABLED=1\n",
        "```\n",
        "\n",
        "などで環境変数を有効化した上で再実行してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prometheus メトリクスレイヤの動作確認\n",
        "\n",
        "次に、`nf_monitoring.prometheus_metrics` を使って、\n",
        "\n",
        "- メトリクス HTTP サーバの起動\n",
        "- run 開始 / 終了メトリクスの記録\n",
        "- 学習中の loss メトリクスの更新\n",
        "\n",
        "を簡単に試します。\n",
        "\n",
        "> ⚠️ 注意: すでに同じポートで Prometheus サーバが動いている環境ではポート衝突が起こり得ます。  \n",
        "> エラーになった場合でも、ライブラリ側で例外を握りつぶす設計にしてあるため、ノートブックは継続実行されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recorded metrics for DemoModel / backend=ray.\n",
            "Prometheus が動いていれば http://localhost:8000/metrics から確認できます。\n"
          ]
        }
      ],
      "source": [
        "from nf_monitoring import prometheus_metrics as pm\n",
        "\n",
        "# メトリクスサーバを起動（複数回呼んでも OK）\n",
        "pm.init_metrics_server(port=8000)\n",
        "\n",
        "# ダミーの run を 1 回記録してみる\n",
        "pm.observe_run_start(model_name=\"DemoModel\", backend=\"ray\")\n",
        "pm.observe_train_step(\"DemoModel\", \"ray\", train_loss=0.5, val_loss=0.6)\n",
        "pm.observe_run_end(\"DemoModel\", \"ray\", status=\"finished\", duration_seconds=1.23, resource_after=None)\n",
        "\n",
        "print(\"Recorded metrics for DemoModel / backend=ray.\")\n",
        "print(\"Prometheus が動いていれば http://localhost:8000/metrics から確認できます。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. リソースモニタの確認\n",
        "\n",
        "`nf_monitoring.resource_monitor.collect_resource_snapshot()` を使うと、CPU やメモリ使用状況を JSON 風の dict で取得できます。\n",
        "\n",
        "- `psutil` が入っていない環境でも、最低限の情報（タイムスタンプ・プラットフォーム）は返るようになっています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'timestamp': 1763022233.8812644,\n",
              " 'platform': 'Windows-10-10.0.26200-SP0',\n",
              " 'cpu_percent': 15.1,\n",
              " 'memory_total': 8258199552,\n",
              " 'memory_used': 7110676480,\n",
              " 'memory_percent': 86.1,\n",
              " 'process_rss': 218845184,\n",
              " 'process_vms': 938889216}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nf_monitoring.resource_monitor import collect_resource_snapshot\n",
        "\n",
        "snapshot = collect_resource_snapshot()\n",
        "snapshot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ドリフト / 異常検知 / モデル比較統計の確認\n",
        "\n",
        "ここでは `nf_analysis` の 3 つのモジュールをまとめて試します:\n",
        "\n",
        "- `drift.compute_univariate_drift`, `compute_dataframe_drift`\n",
        "- `anomaly.detect_zscore_anomalies`\n",
        "- `model_stats.diebold_mariano`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Univariate drift ===\n",
            "{'mean_diff': 0.5313533458785565, 'std_ratio': 1.196018087487258, 'kl_div': 0.19150964832285422}\n",
            "\n",
            "=== DataFrame drift ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>column_name</th>\n",
              "      <th>mean_diff</th>\n",
              "      <th>std_ratio</th>\n",
              "      <th>kl_div</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>x</td>\n",
              "      <td>-0.075297</td>\n",
              "      <td>0.974085</td>\n",
              "      <td>0.311015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>y</td>\n",
              "      <td>0.531353</td>\n",
              "      <td>1.196018</td>\n",
              "      <td>0.191510</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  column_name  mean_diff  std_ratio    kl_div\n",
              "0           x  -0.075297   0.974085  0.311015\n",
              "1           y   0.531353   1.196018  0.191510"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Anomaly detection ===\n",
            "Detected indices: [100] ... (total: 1 )\n",
            "\n",
            "=== Diebold-Mariano test ===\n",
            "{'stat': -8.358078814808186, 'p_value': 0.0}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nf_analysis import drift, anomaly, model_stats\n",
        "\n",
        "# --- 6-1. ドリフト計算 ---\n",
        "rng = np.random.RandomState(42)\n",
        "base = pd.Series(rng.normal(loc=0.0, scale=1.0, size=500))\n",
        "current = pd.Series(rng.normal(loc=0.5, scale=1.2, size=500))\n",
        "\n",
        "univar = drift.compute_univariate_drift(base, current)\n",
        "print(\"=== Univariate drift ===\")\n",
        "print(univar)\n",
        "\n",
        "base_df = pd.DataFrame({\"y\": base, \"x\": rng.normal(size=500)})\n",
        "current_df = pd.DataFrame({\"y\": current, \"x\": rng.normal(size=500)})\n",
        "df_drift = drift.compute_dataframe_drift(base_df, current_df)\n",
        "print(\"\\n=== DataFrame drift ===\")\n",
        "display(df_drift)\n",
        "\n",
        "# --- 6-2. 残差の異常検知 ---\n",
        "residuals = pd.Series(\n",
        "    np.concatenate([rng.normal(scale=1.0, size=100), np.array([10.0]), rng.normal(scale=1.0, size=100)])\n",
        ")\n",
        "anom = anomaly.detect_zscore_anomalies(residuals, threshold=3.0)\n",
        "print(\"\\n=== Anomaly detection ===\")\n",
        "print(\"Detected indices:\", anom[\"indices\"][:10], \"... (total:\", len(anom[\"indices\"]), \")\")\n",
        "\n",
        "# --- 6-3. Diebold-Mariano 検定 ---\n",
        "true_signal = rng.normal(size=300)\n",
        "err1 = true_signal + rng.normal(scale=0.5, size=300)  # 良いモデル\n",
        "err2 = true_signal + rng.normal(scale=1.5, size=300)  # 悪いモデル\n",
        "\n",
        "dm_result = model_stats.diebold_mariano(pd.Series(err1), pd.Series(err2), h=1, loss=\"mse\")\n",
        "print(\"\\n=== Diebold-Mariano test ===\")\n",
        "print(dm_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. HTML レポート生成と成果物確認\n",
        "\n",
        "`nf_reports.html_reporter.render_run_report` を使って、\n",
        "\n",
        "- テンプレートディレクトリに簡単な `run_report.html` を用意\n",
        "- ダミーの run 情報・メトリクスを埋め込んだ HTML を生成\n",
        "- 生成されたファイルのパスと内容を確認\n",
        "\n",
        "という流れを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated report path: c:\\nf\\nf_loto_webui\\demo_reports\\run_123_report.html\n",
            "\n",
            "--- Preview (first 500 chars) ---\n",
            "<!DOCTYPE html>\n",
            "<html>\n",
            "  <body>\n",
            "    <h1>Run Report: 123</h1>\n",
            "    <p>Model: DemoModel</p>\n",
            "    <h2>Metrics</h2>\n",
            "    <ul>\n",
            "    \n",
            "      <li>mse = 0.123</li>\n",
            "    \n",
            "      <li>smape = 5.6</li>\n",
            "    \n",
            "    </ul>\n",
            "    <p>Generated at: 2025-11-13T08:24:02.244721</p>\n",
            "  </body>\n",
            "</html>\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "from nf_reports import html_reporter\n",
        "\n",
        "# 一時的なテンプレートディレクトリ・出力先をプロジェクト配下に作る\n",
        "reports_root = PROJECT_ROOT / \"demo_reports\"\n",
        "templates_dir = reports_root / \"templates\"\n",
        "templates_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "template_path = templates_dir / \"run_report.html\"\n",
        "\n",
        "# シンプルな Jinja2 テンプレート\n",
        "template_path.write_text(\n",
        "    '''<!DOCTYPE html>\n",
        "<html>\n",
        "  <body>\n",
        "    <h1>Run Report: {{ run.run_id }}</h1>\n",
        "    <p>Model: {{ run.model_name }}</p>\n",
        "    <h2>Metrics</h2>\n",
        "    <ul>\n",
        "    {% for row in metrics %}\n",
        "      <li>{{ row.name }} = {{ row.value }}</li>\n",
        "    {% endfor %}\n",
        "    </ul>\n",
        "    <p>Generated at: {{ generated_at }}</p>\n",
        "  </body>\n",
        "</html>\n",
        "''',\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "run_info = {\"run_id\": 123, \"model_name\": \"DemoModel\"}\n",
        "metrics_df = pd.DataFrame([{\"name\": \"mse\", \"value\": 0.123}, {\"name\": \"smape\", \"value\": 5.6}])\n",
        "\n",
        "output_path = reports_root / \"run_123_report.html\"\n",
        "result_path = html_reporter.render_run_report(\n",
        "    template_dir=templates_dir,\n",
        "    output_path=output_path,\n",
        "    run_info=run_info,\n",
        "    metrics_df=metrics_df,\n",
        ")\n",
        "\n",
        "print(\"Generated report path:\", result_path)\n",
        "print(\"\\n--- Preview (first 500 chars) ---\")\n",
        "print(result_path.read_text(encoding=\"utf-8\")[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "上記セルを実行すると、`PROJECT_ROOT/demo_reports/` 配下に HTML レポートが生成されます。  \n",
        "ブラウザで開くことで、Streamlit からダウンロードする時の挙動に近い形を確認できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. メタデータテーブル DDL の確認\n",
        "\n",
        "`nf_metadata.schema_definitions` には、メタデータ用テーブル群の DDL が Python 文字列として定義されています。  \n",
        "ここではその内容を確認し、Postgres への適用コードの雛形も示します。\n",
        "\n",
        "> ⚠️ DB 接続は環境依存なので、このノートブックでは **実際の接続・適用はコメントアウト** してあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature sets table name: nf_feature_sets\n",
            "Datasets table name    : nf_datasets\n",
            "Model registry table   : nf_model_registry\n",
            "Drift metrics table    : nf_drift_metrics\n",
            "Residual anomalies tbl : nf_residual_anomalies\n",
            "Reports table          : nf_reports\n",
            "\n",
            "--- DDL (excerpt, first 500 chars) ---\n",
            "\n",
            "-- Metadata tables for NeuralForecast MLOps extensions\n",
            "\n",
            "CREATE TABLE IF NOT EXISTS nf_feature_sets (\n",
            "    id              SERIAL PRIMARY KEY,\n",
            "    name            TEXT NOT NULL,\n",
            "    description     TEXT,\n",
            "    created_at      TIMESTAMPTZ NOT NULL DEFAULT NOW()\n",
            ");\n",
            "\n",
            "CREATE TABLE IF NOT EXISTS nf_datasets (\n",
            "    id              SERIAL PRIMARY KEY,\n",
            "    table_name      TEXT NOT NULL,\n",
            "    loto            TEXT NOT NULL,\n",
            "    unique_ids      TEXT[] NOT NULL,\n",
            "    ds_start        DATE,\n",
            "    ds_end          DATE\n"
          ]
        }
      ],
      "source": [
        "from nf_metadata import schema_definitions as schemas\n",
        "\n",
        "print(\"Feature sets table name:\", schemas.NF_FEATURE_SETS_TABLE)\n",
        "print(\"Datasets table name    :\", schemas.NF_DATASETS_TABLE)\n",
        "print(\"Model registry table   :\", schemas.NF_MODEL_REGISTRY_TABLE)\n",
        "print(\"Drift metrics table    :\", schemas.NF_DRIFT_METRICS_TABLE)\n",
        "print(\"Residual anomalies tbl :\", schemas.NF_RESIDUAL_ANOMALIES_TABLE)\n",
        "print(\"Reports table          :\", schemas.NF_REPORTS_TABLE)\n",
        "\n",
        "print(\"\\n--- DDL (excerpt, first 500 chars) ---\")\n",
        "ddl = schemas.get_extend_metadata_ddl()\n",
        "print(ddl[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "OperationalError",
          "evalue": "could not translate host name \"host\" to address: Name or service not known\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpsycopg2\u001b[39;00m\n\u001b[32m      6\u001b[39m dsn = \u001b[33m\"\u001b[39m\u001b[33mpostgresql://user:password@host:5432/dbname\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpsycopg2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m conn.cursor() \u001b[38;5;28;01mas\u001b[39;00m cur:\n\u001b[32m      9\u001b[39m         cur.execute(ddl)  \u001b[38;5;66;03m# schemas.get_extend_metadata_ddl() で取得した DDL\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\psycopg2\\__init__.py:135\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    132\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    134\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    137\u001b[39m     conn.cursor_factory = cursor_factory\n",
            "\u001b[31mOperationalError\u001b[39m: could not translate host name \"host\" to address: Name or service not known\n"
          ]
        }
      ],
      "source": [
        "# ▼オプション: 実際に Postgres に DDL を適用するサンプルコード\n",
        "# 実環境に合わせて DSN を設定し、コメントアウトを外してください。\n",
        "\n",
        "import psycopg2\n",
        "\n",
        "dsn = \"postgresql://user:password@host:5432/dbname\"\n",
        "with psycopg2.connect(dsn) as conn:\n",
        "    with conn.cursor() as cur:\n",
        "        cur.execute(ddl)  # schemas.get_extend_metadata_ddl() で取得した DDL\n",
        "    conn.commit()\n",
        "\n",
        "print(\"Metadata tables created/updated successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "テーブル作成後は、普段の SQL クライアント（psql / DBeaver / DataGrip など）や  \n",
        "既存の `nf_model_runs` テーブルと JOIN して、\n",
        "\n",
        "- モデルレジストリ（`nf_model_registry`）\n",
        "- ドリフト結果（`nf_drift_metrics`）\n",
        "- 残差異常（`nf_residual_anomalies`）\n",
        "- レポートメタ情報（`nf_reports`）\n",
        "\n",
        "を確認・活用できるようになります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. pytest による一括テスト実行（オプション）\n",
        "\n",
        "最後に、今回追加したテストを含めて `pytest` をノートブック上から実行する方法を示します。\n",
        "\n",
        "> ⚠️ 注意:  \n",
        "> - すでに CLI から `pytest` を回している場合は再実行は必須ではありません。  \n",
        "> - 実行には多少時間がかかる場合があります。  \n",
        "> - プロジェクトルート直下で `pytest` を実行する前提です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Return code: 0\n",
            "\n",
            "=== pytest stdout ===\n",
            "\n",
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                       [100%]\u001b[0m\n",
            "\u001b[33m============================== warnings summary ===============================\u001b[0m\n",
            "..\\..\\Users\\hashimoto.ryohei\\Miniconda3\\envs\\kaiseki\\Lib\\site-packages\\mlflow\\gateway\\config.py:64\n",
            "  C:\\Users\\hashimoto.ryohei\\Miniconda3\\envs\\kaiseki\\Lib\\site-packages\\mlflow\\gateway\\config.py:64: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
            "    @validator(\"togetherai_api_key\", pre=True)\n",
            "\n",
            "..\\..\\Users\\hashimoto.ryohei\\Miniconda3\\envs\\kaiseki\\Lib\\site-packages\\mlflow\\gateway\\config.py:78\n",
            "  C:\\Users\\hashimoto.ryohei\\Miniconda3\\envs\\kaiseki\\Lib\\site-packages\\mlflow\\gateway\\config.py:78: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
            "    @validator(\"cohere_api_key\", pre=True)\n",
            "\n",
            "..\\..\\Users\\hashimoto.ryohei\\Miniconda3\\envs\\kaiseki\\Lib\\site-packages\\mlflow\\gateway\\config.py:86\n",
            "  C:\\Users\\hashimoto.ryohei\\Miniconda3\\envs\\kaiseki\\Lib\\site-packages\\mlflow\\gateway\\config.py:86: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migr\n",
            "\n",
            "=== pytest stderr ===\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# このセルは任意です。実行するとプロジェクト全体の pytest が走ります。\n",
        "# 実行したくない場合はスキップしてください。\n",
        "\n",
        "import subprocess\n",
        "\n",
        "result = subprocess.run(\n",
        "    [\"pytest\", \"-q\"],\n",
        "    cwd=str(PROJECT_ROOT),\n",
        "    capture_output=True,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "print(\"Return code:\", result.returncode)\n",
        "print(\"\\n=== pytest stdout ===\\n\")\n",
        "print(result.stdout[:2000])  # 長過ぎる場合は先頭だけ表示\n",
        "print(\"\\n=== pytest stderr ===\\n\")\n",
        "print(result.stderr[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. まとめ\n",
        "\n",
        "このノートブックでは、\n",
        "\n",
        "1. 追加モジュールのディレクトリ構成と存在確認  \n",
        "2. W&B / MLflow ロガーの no-op / ダミー動作確認  \n",
        "3. Prometheus メトリクス・リソースモニタの確認  \n",
        "4. ドリフト / 異常検知 / DM 検定のサンプル実行  \n",
        "5. HTML レポート生成と成果物ファイルの確認  \n",
        "6. メタデータテーブル DDL の確認と Postgres 適用の雛形  \n",
        "7. pytest による一括テスト実行方法  \n",
        "\n",
        "を一通りなぞりました。\n",
        "\n",
        "このノートブックをベースに、\n",
        "\n",
        "- 本番用の DSN を差し込んでメタテーブルを実際に作る\n",
        "- 既存の `nf_model_runs` / `nf_loto%` テーブルと JOIN して分析する\n",
        "- 生成された HTML レポートをメール送信・S3 保存などに発展させる\n",
        "\n",
        "といった「運用寄りの確認フロー」を自分用にカスタマイズしていくと、  \n",
        "時系列 MLOps 基盤としての整備状況を **一括でドキュメント＋検証** できるようになります。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "kaiseki",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
